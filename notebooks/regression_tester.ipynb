{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression tester\n",
    "\n",
    "A version in jupyter notebook form to save time loading large csv files.\n",
    "\n",
    "Regression test to compare two versions of outputs\n",
    "Reads two csv files, old and new\n",
    "Selects the columns of interest\n",
    "Joins old and new on key columns, outer\n",
    "Checks which records are in old only (left), new only (right) or both\n",
    "Compares if the old and new values are the same within tolerance\n",
    "Saves the ouotputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation file location and name\n",
    "common_dir = \"R:\\\\BERD Results System Development 2023\\\\DAP_emulation\\\\\"\n",
    "\n",
    "input_dir_new = \"outputs\\\\output_long_form\"\n",
    "input_dir_new = \"outputs\\\\output_gb_sas\"\n",
    "\n",
    "pref_old = \"Frozen_Base_Data\"\n",
    "suff_old = \"_2024-02-05_v471.csv\"\n",
    "\n",
    "pref_new = \"output_gb_sas\"\n",
    "suff_new = \"_2024-02-05_v471.csv\"\n",
    "\n",
    "# Output folder for all schemas\n",
    "out_dir = \"D:\\\\coding_projectsrandd_test_data\\\\\"\n",
    "\n",
    "# Columns to select\n",
    "key_cols_old = [\"reference\", \"200\", \"201\"]\n",
    "key_cols_new = [\"reference\", \"instance\", \"200\", \"201\"]\n",
    "value_col = \"211\"\n",
    "other_cols = [\"status\", \"imp_marker\",  \"602\", \"referencepostcode\", \"postcodes_harmonised\"]\n",
    "tolerance = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in_dir = common_dir\n",
    "read_in_path_old = os.path.join(common_dir, input_dir_old, pref_old + suff_old)\n",
    "df_old = pd.read_csv(read_in_path_old)\n",
    "\n",
    "\n",
    "# Remove \"index\" column if it exists\n",
    "if \"index\" in df_old.columns:\n",
    "    df_old = df.drop(\"index\", axis=1)\n",
    "\n",
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the top 100 rows, inferrring the schema from csv\n",
    "read_in_dir = common_dir\n",
    "read_in_path_new = os.path.join(common_dir, input_dir_new, pref_new + suff_new)\n",
    "df_new = pd.read_csv(read_in_path_new)\n",
    "\n",
    "\n",
    "# Remove \"index\" column if it exists\n",
    "if \"index\" in df_new.columns:\n",
    "    df_new = df.drop(\"index\", axis=1)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter conditions\n",
    "def get_mask(df:pd.DataFrame, col:str, values:list):\n",
    "    return df[col].isin(values)\n",
    "\n",
    "def filter_df(df:pd.DataFrame, col:str, values:list):\n",
    "    return df.copy().loc[df[col].isin(values)]\n",
    "\n",
    "# filter_df(df_old, \"formtype\", [1]).head()\n",
    "\n",
    "# # Filter good statuses only\n",
    "# imp_markers_to_keep = [\"TMI\", \"CF\", \"MoR\", \"constructed\"]\n",
    "# df_old = filter_df(df_old, \"imp_marker\", imp_markers_to_keep)\n",
    "# df_new = filter_df(df_new, \"imp_marker\", imp_markers_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sizes\n",
    "print(f\"Old size: {df_old.shape}\")\n",
    "print(f\"New size: {df_new.shape}\")\n",
    "\n",
    "#%% Join\n",
    "df_merge = df_old.merge(\n",
    "    df_new,\n",
    "    on=key_cols,\n",
    "    how=\"outer\",\n",
    "    suffixes=(\"_old\", \"_new\"),\n",
    "    indicator=True\n",
    ")\n",
    "#%% Compare the values\n",
    "df_merge[\"value_different\"] = (\n",
    "    (df_merge[value_col + \"_old\"] - df_merge[value_col + \"_new\"])**2 > tolerance**2\n",
    ")\n",
    "\n",
    "# %% Save output\n",
    "write_csv(out_fol + out_file, df_merge)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, the schema is empty\n",
    "base_str = \"\"\n",
    "add_in_str = f'[{col}]\\nold_name = \"{col}\"\\nDeduced_Data_Type = \"{schema[col]}\"\\n\\n'\n",
    "\n",
    "\n",
    "# Iterate through columns, adding to the string which will be written to toml\n",
    "for col in schema:\n",
    "    base_str += add_in_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the schema toml file\n",
    "mypath = os.path.join(out_dir, pref + \"_schema.toml\")\n",
    "text_file = open(mypath, \"w\")\n",
    "text_file.write(S)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
